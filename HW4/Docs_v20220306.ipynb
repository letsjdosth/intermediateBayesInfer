{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document briefly explains classes 'MCMC_MH', 'MCMC_Diag', 'MCMC_Gibbs', and 'NewtonUnconstrained'.\n",
    "\n",
    "This is written based on 6ebb6581dc296ae43d5824bd78d12d934f3e4e84 (Mar 5, 2022) of letsjdosth/intermediateBayesInfer on github.\n",
    "\n",
    "Link: https://github.com/letsjdosth/intermediateBayesInfer/tree/main/HW4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency\n",
    "\n",
    "- Python 3.8.7 64 bit\n",
    "    - numpy 1.19.5\n",
    "    - matplotlib 3.3.3\n",
    "    - (for newton method,) scipy 1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python standard libraries\n",
    "import time\n",
    "import csv\n",
    "from math import log\n",
    "from random import seed, uniform\n",
    "from statistics import mean, variance\n",
    "\n",
    "# other libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class MCMC_MH_Core.MCMC_MH(log_target_pdf, log_proposal_pdf, proposal_sampler, initial, random_seed)\n",
    "An object for MCMC Metropolis-Hastings algorithm.\n",
    "\n",
    "Arguments for constructor '\\_\\_init\\_\\_'\n",
    "\n",
    "- log_target_pdf(eval_pt, ...) and log_proposal_pdf(from_smpl, to_smpl) should be function objects returning corresponding 'float' values as their name.\n",
    "- proposal_sampler(last, ...) should be a function object to return a list- or tuple-type sample under the condition that last sample is 'last'\n",
    "- initial: list. initial value for MCMC-MH iteration.\n",
    "- random seed: random seed (to replicate a result).\n",
    "\n",
    "Methods\n",
    "\n",
    "- MCMC_MH.sampler()\n",
    "    - Main algorithm of MCMC-MH. It runs one iteration of MCMC-MH using functions that are given by constructor.\n",
    "- MCMC_MH.generate_samples(num_samples, pid=None, verbose=True, print_iter_cycle=500)\n",
    "    - It iterates the 'sampler' function with printing about some iteration information to console.\n",
    "- MCMC_MH.log_r_calculator(candid, last)\n",
    "    - Acceptance ratio. it is $log\\frac{(f(x)q(x|x_{t-1})}{f(x_{t-1})q(x_{t-1}|x)}$ where f: target density, q: proposal density\n",
    "    - This is used in sampler() method. Do not use it directly.\n",
    "- MCMC_MH.write_samples(filename)\n",
    "    - It saves drawn samples to csv file.\n",
    "\n",
    "Attributes\n",
    "\n",
    "- MCMC_MH.MC_sample\n",
    "    - MCMC sample list\n",
    "- MCMC_MH.num_total_iters\n",
    "    - total number of iteration\n",
    "- MCMC_MH.num_accept\n",
    "    - total number of acceptance\n",
    "- MCMC_MH.random_seed\n",
    "    - random seed set by constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCMC_MH:\n",
    "    def __init__(self, log_target_pdf, log_proposal_pdf, proposal_sampler, initial, random_seed):\n",
    "        self.log_target_pdf = log_target_pdf #arg (smpl)\n",
    "        self.log_proposal_pdf = log_proposal_pdf #arg (from_smpl, to_smpl)\n",
    "        self.proposal_sampler = proposal_sampler #function with argument (smpl)\n",
    "        \n",
    "        self.initial = initial\n",
    "        \n",
    "        self.MC_sample = [initial]\n",
    "\n",
    "        self.num_total_iters = 0\n",
    "        self.num_accept = 0\n",
    "\n",
    "        self.random_seed = random_seed\n",
    "        seed(random_seed)\n",
    "        \n",
    "\n",
    "    def log_r_calculator(self, candid, last):\n",
    "        log_r = (self.log_target_pdf(candid) - self.log_proposal_pdf(from_smpl=last, to_smpl=candid) - \\\n",
    "             self.log_target_pdf(last) + self.log_proposal_pdf(from_smpl=candid, to_smpl=last))\n",
    "        return log_r\n",
    "\n",
    "    def sampler(self):\n",
    "        last = self.MC_sample[-1]\n",
    "        candid = self.proposal_sampler(last) #기존 state 집어넣게\n",
    "        unif_sample = uniform(0, 1)\n",
    "        log_r = self.log_r_calculator(candid, last)\n",
    "        # print(log(unif_sample), log_r) #for debug\n",
    "        if log(unif_sample) < log_r:\n",
    "            self.MC_sample.append(candid)\n",
    "            self.num_total_iters += 1\n",
    "            self.num_accept += 1\n",
    "        else:\n",
    "            self.MC_sample.append(last)\n",
    "            self.num_total_iters += 1\n",
    "\n",
    "    def generate_samples(self, num_samples, pid=None, verbose=True, print_iter_cycle=500):\n",
    "        start_time = time.time()\n",
    "        for i in range(num_samples):\n",
    "            self.sampler()\n",
    "            \n",
    "            if i==100 and verbose:\n",
    "                elap_time_head_iter = time.time()-start_time\n",
    "                estimated_time = (num_samples/100)*elap_time_head_iter\n",
    "                print(\"estimated running time: \", estimated_time//60, \"min \", estimated_time%60, \"sec\")\n",
    "\n",
    "            if i%print_iter_cycle == 0 and verbose and pid is not None:\n",
    "                print(\"pid:\",pid,\" iteration\", i, \"/\", num_samples)\n",
    "            elif i%print_iter_cycle == 0 and verbose and pid is None:\n",
    "                print(\"iteration\", i, \"/\", num_samples)\n",
    "        elap_time = time.time()-start_time\n",
    "        \n",
    "        if pid is not None and verbose:\n",
    "            print(\"pid:\",pid, \"iteration\", num_samples, \"/\", num_samples, \" done! (elapsed time for execution: \", elap_time//60,\"min \", elap_time%60,\"sec)\")\n",
    "            print(\"acceptance rate: \", round(self.num_accept / self.num_total_iters, 4))\n",
    "        elif pid is None and verbose:\n",
    "            print(\"iteration\", num_samples, \"/\", num_samples, \" done! (elapsed time for execution: \", elap_time//60,\"min \", elap_time%60,\"sec)\")\n",
    "            print(\"acceptance rate: \", round(self.num_accept / self.num_total_iters, 4))\n",
    "\n",
    "        \n",
    "    def write_samples(self, filename: str):\n",
    "        with open(filename + '.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for sample in self.MC_sample:\n",
    "                csv_row = sample\n",
    "                writer.writerow(csv_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class MCMC_MH_Core.MCMC_Diag()\n",
    "An object to diagnose samples generated from MCMC Metropolis-Hastings algorithm.\n",
    "\n",
    "\n",
    "Methods\n",
    "\n",
    "- Setters:\n",
    "    - MCMC_Diag.set_mc_samples_from_list(mc_sample)\n",
    "        - set MCMC samples to MCMC_Diag.MC_sample from a python list (or list-like objects)\n",
    "    - MCMC_Diag.set_mc_sample_from_MCMC_MH(inst_MCMC_MH)    \n",
    "        - set MCMC samples to MCMC_Diag.MC_sample from a MCMC_MH object (or any object that has an attribute named 'MC_sample')\n",
    "    - MCMC_Diag.set_mc_sample_from_csv(file_name)\n",
    "        - set MCMC samples to MCMC_Diag.MC_sample from a csv file\n",
    "\n",
    "- Writers:\n",
    "    - MCMC_Diag.write_samples(filename)    \n",
    "        - It saves MCMC_Diag.MC_sample list as a csv file\n",
    "\n",
    "- MCMC post-worker\n",
    "    - MCMC_Diag.burnin(num_burn_in)\n",
    "        - cut first 'num_bun_in' samples\n",
    "    - MCMC_Diag.thinning(lag)\n",
    "        - pick every one sample out of 'lag' and save it to MCMC_Diag.MC_sample\n",
    "\n",
    "- Getters:\n",
    "    - MCMC_Diag.get_specific_dim_samples(dim_idx)\n",
    "        - get a list of designated dimension's marginal MC_samples\n",
    "    - MCMC_Diag.get_sample_mean(self)\n",
    "        - get a sample mean of each dimension of MC_samples\n",
    "    - MCMC_Diag.get_sample_var(self)\n",
    "        - get a sample variance of each dimension of MC_samples\n",
    "    - MCMC_Diag.get_sample_quantile(quantile_list)\n",
    "        - quantile_list should be python list (or list-like object)\n",
    "        - get sample quantiles of each dimension of MC_samples\n",
    "    - MCMC_Diag.get_autocorr(dim_idx, maxLag)\n",
    "        - get a sample autocorrelation list (lag 1~maxLag) of designated dimension\n",
    "    - MCMC_Diag.effective_sample_size(dim_idx, sum_lags=30)\n",
    "        - get the effective sample size. It is implemented following the formula of BDA3(Gelman et al,), setting the number of chains to 1.\n",
    "\n",
    "- Visualizers\n",
    "    - MCMC_Diag.show_traceplot_specific_dim(dim_idx, show=False)\n",
    "    - MCMC_Diag.show_traceplot(figure_grid_dim, show=True)\n",
    "    - MCMC_Diag.show_hist_specific_dim(dim_idx, show=False)\n",
    "    - MCMC_Diag.show_hist(figure_grid_dim, show=True)\n",
    "    - MCMC_Diag.show_acf_specific_dim(dim_idx, maxLag, show=False)\n",
    "    - MCMC_Diag.show_acf(maxLag, figure_grid_dim, show=True)\n",
    "    - MCMC_Diag.show_scatterplot(dim_idx_horizontal, dim_idx_vertical, show=True)\n",
    "    \n",
    "\n",
    "Attributes\n",
    "\n",
    "- self.MC_sample: MCMC samples that set in the instance\n",
    "- self.num_dim: sample dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCMC_Diag:\n",
    "    def __init__(self):\n",
    "        self.MC_sample = []\n",
    "        self.num_dim = None\n",
    "    \n",
    "    def set_mc_samples_from_list(self, mc_sample):\n",
    "        self.MC_sample = mc_sample\n",
    "        self.num_dim = len(mc_sample[0])\n",
    "    \n",
    "    def set_mc_sample_from_MCMC_MH(self, inst_MCMC_MH):\n",
    "        self.MC_sample = inst_MCMC_MH.MC_sample\n",
    "        self.num_dim = len(inst_MCMC_MH.MC_sample[0])\n",
    "\n",
    "    def set_mc_sample_from_csv(self, file_name):\n",
    "        with open(file_name + '.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for csv_row in reader:\n",
    "                csv_row = [float(elem) for elem in csv_row]\n",
    "                self.MC_sample(csv_row)\n",
    "        self.num_dim = len(self.MC_sample[0])\n",
    "\n",
    "    def write_samples(self, filename: str):\n",
    "        with open(filename + '.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for sample in self.MC_sample:\n",
    "                csv_row = sample\n",
    "                writer.writerow(csv_row)\n",
    "\n",
    "    def burnin(self, num_burn_in):\n",
    "        self.MC_sample = self.MC_sample[num_burn_in-1:]\n",
    "\n",
    "    def thinning(self, lag):\n",
    "        self.MC_sample = self.MC_sample[::lag]\n",
    "    \n",
    "    def get_specific_dim_samples(self, dim_idx):\n",
    "        if dim_idx >= self.num_dim:\n",
    "            raise ValueError(\"dimension index should be lower than number of dimension. note that index starts at 0\")\n",
    "        return [smpl[dim_idx] for smpl in self.MC_sample]\n",
    "    \n",
    "    def get_sample_mean(self):\n",
    "        mean_vec = []\n",
    "        for i in range(self.num_dim):\n",
    "            ith_dim_samples = self.get_specific_dim_samples(i)\n",
    "            mean_vec.append(mean(ith_dim_samples))\n",
    "        return mean_vec\n",
    "\n",
    "    def get_sample_var(self):\n",
    "        var_vec = []\n",
    "        for i in range(self.num_dim):\n",
    "            ith_dim_samples = self.get_specific_dim_samples(i)\n",
    "            var_vec.append(variance(ith_dim_samples))\n",
    "        return var_vec\n",
    "\n",
    "    def get_sample_quantile(self, quantile_list):\n",
    "        quantile_vec = []\n",
    "        for i in range(self.num_dim):\n",
    "            ith_dim_samples = self.get_specific_dim_samples(i)\n",
    "            quantiles = [np.quantile(ith_dim_samples, q) for q in quantile_list]\n",
    "            quantile_vec.append(quantiles)\n",
    "        return quantile_vec\n",
    "\n",
    "    def show_traceplot_specific_dim(self, dim_idx, show=False):\n",
    "        traceplot_data = self.get_specific_dim_samples(dim_idx)\n",
    "        plt.ylabel(str(dim_idx)+\"th dim\")\n",
    "        plt.plot(range(len(traceplot_data)), traceplot_data)\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    def show_traceplot(self, figure_grid_dim, show=True):\n",
    "        grid_column= figure_grid_dim[0]\n",
    "        grid_row = figure_grid_dim[1]\n",
    "\n",
    "        plt.figure(figsize=(5*grid_column, 3*grid_row))\n",
    "        for i in range(self.num_dim):\n",
    "            plt.subplot(grid_row, grid_column, i+1)\n",
    "            self.show_traceplot_specific_dim(i)\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    \n",
    "    def show_hist_specific_dim(self, dim_idx, show=False):\n",
    "        hist_data = self.get_specific_dim_samples(dim_idx)\n",
    "        plt.ylabel(str(dim_idx)+\"th dim\")\n",
    "        plt.hist(hist_data, bins=100)\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    def show_hist(self, figure_grid_dim, show=True):\n",
    "        grid_column= figure_grid_dim[0]\n",
    "        grid_row = figure_grid_dim[1]\n",
    "       \n",
    "        plt.figure(figsize=(5*grid_column, 3*grid_row))\n",
    "        for i in range(self.num_dim):\n",
    "            plt.subplot(grid_row, grid_column, i+1)\n",
    "            self.show_hist_specific_dim(i)\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    def get_autocorr(self, dim_idx, maxLag):\n",
    "        y = self.get_specific_dim_samples(dim_idx)\n",
    "        acf = []\n",
    "        y_mean = mean(y)\n",
    "        y = [elem - y_mean  for elem in y]\n",
    "        n_var = sum([elem**2 for elem in y])\n",
    "        for k in range(maxLag+1):\n",
    "            N = len(y)-k\n",
    "            n_cov_term = 0\n",
    "            for i in range(N):\n",
    "                n_cov_term += y[i]*y[i+k]\n",
    "            acf.append(n_cov_term / n_var)\n",
    "        return acf\n",
    "    \n",
    "    def show_acf_specific_dim(self, dim_idx, maxLag, show=False):\n",
    "        grid = [i for i in range(maxLag+1)]\n",
    "        acf = self.get_autocorr(dim_idx, maxLag)\n",
    "        plt.ylim([-1,1])\n",
    "        plt.ylabel(str(dim_idx)+\"th dim\")\n",
    "        plt.bar(grid, acf, width=0.3)\n",
    "        plt.axhline(0, color=\"black\", linewidth=0.8)\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    def show_acf(self, maxLag, figure_grid_dim, show=True):\n",
    "        grid_column= figure_grid_dim[0]\n",
    "        grid_row = figure_grid_dim[1]\n",
    "\n",
    "        plt.figure(figsize=(5*grid_column, 3*grid_row))\n",
    "        for i in range(self.num_dim):\n",
    "            plt.subplot(grid_row, grid_column, i+1)\n",
    "            self.show_acf_specific_dim(i, maxLag)\n",
    "        if show:\n",
    "            plt.show()\n",
    "    \n",
    "    def show_scatterplot(self, dim_idx_horizontal, dim_idx_vertical, show=True):\n",
    "        x = self.get_specific_dim_samples(dim_idx_horizontal)\n",
    "        y = self.get_specific_dim_samples(dim_idx_vertical)\n",
    "        plt.scatter(x, y)\n",
    "        if show:\n",
    "            plt.show()\n",
    "    \n",
    "    def effective_sample_size(self, dim_idx, sum_lags=30):\n",
    "        n = len(self.MC_sample)\n",
    "        auto_corr = self.get_autocorr(dim_idx, sum_lags)\n",
    "        ess = n / (1 + 2*sum(auto_corr))\n",
    "        return ess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class MCMC_Gibbs_Core.MCMC_Gibbs(initial)\n",
    "\n",
    "I tried to design this class as a high-abstract-level tool for a gibbs sampler,\n",
    "but, I failed to make this class to be a satisfactory level, because the structure of full-conditionals are hard to generalize.\n",
    "As a result, it consists of only parts of the MCMC_MH class for now.\n",
    "\n",
    "So, I skip the explanation for this class's methods and attributes. see the MCMC_MH part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MCMC_Gibbs:\n",
    "    def __init__(self, initial):\n",
    "        self.MC_sample = [initial]\n",
    "\n",
    "    def gibbs_sampler(self):\n",
    "        last = self.MC_sample[-1]\n",
    "        new = [x for x in last] #[nu, theta]\n",
    "        #update new\n",
    "\n",
    "        self.MC_sample.append(new)\n",
    "\n",
    "    \n",
    "    def full_conditional_sampler(self, last_param):\n",
    "        new_sample = [x for x in last_param]\n",
    "        #update new\n",
    "\n",
    "        return new_sample\n",
    "\n",
    "\n",
    "    def generate_samples(self, num_samples, pid=None, verbose=True, print_iter_cycle=500):\n",
    "        start_time = time.time()\n",
    "        for i in range(1, num_samples):\n",
    "            self.gibbs_sampler()\n",
    "            \n",
    "            if i==100 and verbose:\n",
    "                elap_time_head_iter = time.time()-start_time\n",
    "                estimated_time = (num_samples/100)*elap_time_head_iter\n",
    "                print(\"estimated running time: \", estimated_time//60, \"min \", estimated_time%60, \"sec\")\n",
    "\n",
    "            if i%print_iter_cycle == 0 and verbose and pid is not None:\n",
    "                print(\"pid:\",pid,\" iteration\", i, \"/\", num_samples)\n",
    "            elif i%print_iter_cycle == 0 and verbose and pid is None:\n",
    "                print(\"iteration\", i, \"/\", num_samples)\n",
    "        elap_time = time.time()-start_time\n",
    "        \n",
    "        if pid is not None and verbose:\n",
    "            print(\"pid:\",pid, \"iteration\", num_samples, \"/\", num_samples, \" done! (elapsed time for execution: \", elap_time//60,\"min \", elap_time%60,\"sec)\")\n",
    "        elif pid is None and verbose:\n",
    "            print(\"iteration\", num_samples, \"/\", num_samples, \" done! (elapsed time for execution: \", elap_time//60,\"min \", elap_time%60,\"sec)\")\n",
    "\n",
    "        \n",
    "    def write_samples(self, filename: str):\n",
    "        with open(filename + '.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for sample in self.MC_sample:\n",
    "                csv_row = sample\n",
    "                writer.writerow(csv_row)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class newton.NewtonUnconstrained(fn_objective, fn_objective_gradient, fn_objective_hessian, fn_objective_domain_indicator = None)\n",
    "\n",
    "This class is an implementation of the Newton method with the backtracking line search (using Cholesky decomposition / pseudo inverse).\n",
    "The reference is: BBV chap 9.5-7. Unconstrained minimization: Newton's method, Self-concordance, Implementation\n",
    "\n",
    "This is part of https://github.com/letsjdosth/convexOptim\n",
    "(I implemented some optimization algorithms last winter, cause I had nothing to do during winter break (no friend, no money, etc..))\n",
    "\n",
    "Constructor arguments\n",
    "\n",
    "- fn_objective(x)\n",
    "    - Python function object. x is numpy.array(-like) type\n",
    "    - R^n to R^1 function. Return objective function value at x\n",
    "- fn_objective_gradient(x)\n",
    "    - Python function object. x is numpy.array(-like) type\n",
    "    - R^n to R^n function. Return gradient vector at x, as a numpy array type\n",
    "- fn_objective_hessian(x)\n",
    "    - Python function object. x is numpy.array(-like) type\n",
    "    - R^n to R^(n*n). Return Hessian matrix at x, as a numpy array\n",
    "- fn_objective_domain_indicator = None\n",
    "    - Python function object. x is numpy.array(-like) type\n",
    "    - Optional, if you want to restrict the domain as a subset of R^n, set a indicator (1: x is in the domain, 0: otherwise)\n",
    "\n",
    "Methods\n",
    "\n",
    "- Hidden methods:\n",
    "    - NewtonUnconstrained._Rn_domain_indicator(eval_pt)\n",
    "    - NewtonUnconstrained._backtracking_line_search(eval_pt, descent_direction, a_slope_flatter_ratio, b_step_shorten_ratio)\n",
    "    - NewtonUnconstrained._l2_norm(vec)\n",
    "    - NewtonUnconstrained._descent_direction_newton_cholesky(eval_pt)\n",
    "    - NewtonUnconstrained._descent_direction_newton_pinv(eval_pt)\n",
    "\n",
    "- Runner:\n",
    "    - NewtonUnconstrained.run_newton_with_backtracking_line_search(starting_pt, tolerance = 0.001, method=\"cholesky\", a_slope_flatter_ratio = 0.2, b_step_shorten_ratio = 0.5)\n",
    "        - It runs the newton method with backtracking line search. \n",
    "        - You may choose the matrix-inversion method among 'cholesky'(Cholesky decomposition) and 'pinv'(Pseudo inverse).\n",
    "        - starting_pt is a starting point. Note that this algorithm only guarantee a local minimum, not the global minimum, so the starting point is important.\n",
    "        - tolerance is the termination condition. Less tolerance gives us more accurate minimum but makes get harder to stop.\n",
    "        - a_slope_flatter_ratio should be 0 < a < 0.5\", and b_step_shorten_ratio should be 0 < b < 1. These are used for backtracking line search algorithm.\n",
    "\n",
    "- Getters:\n",
    "    - NewtonUnconstrained.get_minimizing_sequence()\n",
    "        - It returns the sequence of points in the domain in each newton step. THe first element is initial value, and the last element is local minimum point.\n",
    "    - NewtonUnconstrained.get_minimizing_function_value_sequence()\n",
    "        - It returns the sequence of (target, minimizing) function value at each newton step. the last element is a local minimum value.\n",
    "    - NewtonUnconstrained.get_decrement_sequence()\n",
    "        - It reterns the sequence of decrement values. (It can be used as a measure of distance between a local minimum and the newton step point now.)\n",
    "    - NewtonUnconstrained.get_arg_min()\n",
    "        - It returns the last element of the minimizing sequence.\n",
    "    - NewtonUnconstrained.get_min()\n",
    "        - It returns the last element of the minimizing function value sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg\n",
    "\n",
    "\n",
    "class NewtonUnconstrained:\n",
    "    def __init__(self, fn_objective, fn_objective_gradient, fn_objective_hessian, fn_objective_domain_indicator = None):\n",
    "        \n",
    "        self.objective = fn_objective\n",
    "        self.objective_gradient = fn_objective_gradient\n",
    "        self.objective_hessian = fn_objective_hessian\n",
    "        \n",
    "        \n",
    "        if fn_objective_domain_indicator is not None:\n",
    "            self.objective_domain_indicator = fn_objective_domain_indicator\n",
    "        else:\n",
    "            self.objective_domain_indicator = self._Rn_domain_indicator\n",
    "        \n",
    "        self.minimizing_sequence = []\n",
    "        self.decrement_sequence = []\n",
    "        self.value_sequence = []\n",
    "        \n",
    "\n",
    "    def _Rn_domain_indicator(self, eval_pt):\n",
    "        return True\n",
    "\n",
    "    def _backtracking_line_search(self, eval_pt, descent_direction, \n",
    "            a_slope_flatter_ratio, b_step_shorten_ratio):\n",
    "\n",
    "        if a_slope_flatter_ratio <= 0 or a_slope_flatter_ratio >= 0.5:\n",
    "            raise ValueError(\"a should be 0 < a < 0.5\")\n",
    "        if b_step_shorten_ratio <= 0 or b_step_shorten_ratio >= 1:\n",
    "            raise ValueError(\"b should be 0 < a < 1\")\n",
    "\n",
    "        step_size = 1\n",
    "\n",
    "        while True:\n",
    "            flatten_line_slope = self.objective_gradient(eval_pt) * a_slope_flatter_ratio * step_size\n",
    "            deviation_vec = descent_direction * step_size\n",
    "\n",
    "            objective_fn_value = self.objective(eval_pt + deviation_vec)\n",
    "            flatten_line_value = self.objective(eval_pt) + sum(flatten_line_slope * deviation_vec)\n",
    "\n",
    "            if objective_fn_value < flatten_line_value and self.objective_domain_indicator(eval_pt + deviation_vec):\n",
    "                break\n",
    "            else:\n",
    "                step_size = step_size * b_step_shorten_ratio\n",
    "\n",
    "        return step_size\n",
    "    \n",
    "    def _l2_norm(self, vec):\n",
    "        return (sum(vec**2))**0.5\n",
    "    \n",
    "    def _descent_direction_newton_cholesky(self, eval_pt):\n",
    "        hessian = self.objective_hessian(eval_pt)\n",
    "        neg_gradient = self.objective_gradient(eval_pt) * (-1)\n",
    "        cholesky_lowertri_of_hessian = scipy.linalg.cholesky(hessian, lower=True) # L ; H = L(L^T)\n",
    "        #want: x; Hx = -g\n",
    "        forward_variable = scipy.linalg.solve_triangular(cholesky_lowertri_of_hessian, neg_gradient, lower=True) # w ; Lw = -g\n",
    "        newton_decrement = self._l2_norm(forward_variable)\n",
    "        direction_newton_step = scipy.linalg.solve_triangular(np.transpose(cholesky_lowertri_of_hessian), forward_variable, lower=False) # x ; (L^t)x = w\n",
    "        return (direction_newton_step, newton_decrement)\n",
    "\n",
    "    # later: sparse / band hessian version\n",
    "\n",
    "    def _descent_direction_newton_pinv(self, eval_pt):\n",
    "        hessian = self.objective_hessian(eval_pt)\n",
    "        neg_gradient = self.objective_gradient(eval_pt) * (-1)\n",
    "        direction_newton_step = np.matmul(np.linalg.pinv(hessian), neg_gradient) #pseudo\n",
    "        newton_decrement = np.matmul(np.transpose(neg_gradient), direction_newton_step)\n",
    "        return (direction_newton_step, newton_decrement)\n",
    "\n",
    "    def run_newton_with_backtracking_line_search(self, starting_pt, tolerance = 0.001, \n",
    "                                                method=\"cholesky\", \n",
    "                                                a_slope_flatter_ratio = 0.2, b_step_shorten_ratio = 0.5):\n",
    "        #method : cholesky, pinv (if hessian is singular)\n",
    "        self.minimizing_sequence = [starting_pt]\n",
    "        self.value_sequence = [self.objective(starting_pt)]\n",
    "        self.decrement_sequence = []\n",
    "        num_iter = 0\n",
    "        while True:\n",
    "            eval_pt = self.minimizing_sequence[-1]\n",
    "            if method == \"cholesky\":\n",
    "                descent_direction, decrement = self._descent_direction_newton_cholesky(eval_pt)\n",
    "            elif method == \"pinv\":\n",
    "                descent_direction, decrement = self._descent_direction_newton_pinv(eval_pt)\n",
    "            else:\n",
    "                raise ValueError(\"method should be ['cholesky', 'pinv']\")\n",
    "            self.decrement_sequence.append(decrement)\n",
    "\n",
    "            if (decrement**2) < (tolerance*2):\n",
    "                break\n",
    "\n",
    "            descent_step_size = self._backtracking_line_search(eval_pt, descent_direction, \n",
    "                                    a_slope_flatter_ratio, b_step_shorten_ratio)\n",
    "            next_point = eval_pt + descent_direction * descent_step_size\n",
    "            self.minimizing_sequence.append(next_point)\n",
    "            self.value_sequence.append(self.objective(next_point))\n",
    "            num_iter += 1\n",
    "\n",
    "        print(\"iteration: \", num_iter)\n",
    "    \n",
    "    def get_minimizing_sequence(self):\n",
    "        return self.minimizing_sequence\n",
    "    \n",
    "    def get_minimizing_function_value_sequence(self):\n",
    "        return self.value_sequence\n",
    "\n",
    "    def get_decrement_sequence(self):\n",
    "        return self.decrement_sequence\n",
    "\n",
    "    def get_arg_min(self):\n",
    "        return self.minimizing_sequence[-1]\n",
    "\n",
    "    def get_min(self):\n",
    "        return self.objective(self.minimizing_sequence[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examples\n",
    "#test 1\n",
    "def test_objective1(vec_2dim, gamma = 2):\n",
    "    val = 0.5 * (vec_2dim[0]**2 + gamma * (vec_2dim[1]**2))\n",
    "    return np.array(val)\n",
    "\n",
    "def test_objective1_gradient(vec_2dim, gamma = 2):\n",
    "    grad = (vec_2dim[0], vec_2dim[1] * gamma)\n",
    "    return np.array(grad)\n",
    "\n",
    "def test_objective1_hessian(vec_2dim, gamma = 2):\n",
    "    hessian = [[1, 0],\n",
    "                [0, gamma]]\n",
    "    return np.array(hessian)\n",
    "\n",
    "test_newton_inst1 = NewtonUnconstrained(\n",
    "    test_objective1, \n",
    "    test_objective1_gradient, \n",
    "    test_objective1_hessian)\n",
    "test_newton_inst1.run_newton_with_backtracking_line_search(np.array([1000, 150]), method=\"cholesky\")\n",
    "print(test_newton_inst1.get_minimizing_sequence())\n",
    "print(test_newton_inst1.get_decrement_sequence())\n",
    "print(test_newton_inst1.get_minimizing_function_value_sequence())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
