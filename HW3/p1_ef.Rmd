---
title: "Stat206b hw3 problem1 e/f"
author: "Seokjun Choi"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

Let's start with setting seed and simulating random numbers that will be used as a dataset.

```{r}
set.seed(20220216)
```


# simulate sample (observations)
```{r}
simed_obs_n4 <- rnorm(50, 4, sqrt(10))
simed_obs_n1.5 <- rnorm(50, 1.5, sqrt(10))

par(mfrow=c(1, 2))
hist(simed_obs_n4)
hist(simed_obs_n1.5)
```

# drawing posterior samples

## step 1: get posterior (updated) mixture weight and parameters

Cause we know the analytic form up to normalizing constant, 
(note that this is conjugate mixture model!)
I will draw samples using the posterior kernel and weights.

Let's get the posterior parameters for each mixture and weights.

```{r}
update_weight_normalmixture <- function(prior_weight, mu_vec, x_bar){
    tau = 1
    sigma = sqrt(10)
    n = 50
    new_weight = prior_weight * exp(-0.5*(n/(n*tau^2+sigma^2))*(mu_vec-x_bar)^2)
    new_weight = new_weight/sum(new_weight)
    return(new_weight)
}

update_normalmixture_parameters <- function(mu, x_bar){
    tau = 1
    sigma = sqrt(10)
    n = 50
    scale_param  = 1/((n/sigma^2) + (1/tau^2))
    loc_param = scale_param * (x_bar*n/sigma^2 + mu/tau^2)
    return(c(loc_param, sqrt(scale_param)))
}

prior_weight = c(1/3, 1/3, 1/3)
prior_mu_vec = c(-3,0,3)
posterior_weight_n4 = update_weight_normalmixture(
    prior_weight, prior_mu_vec, mean(simed_obs_n4))
print(round(posterior_weight_n4, 4))

posterior_weight_n1.5 = update_weight_normalmixture(
    prior_weight, prior_mu_vec, mean(simed_obs_n1.5))
print(round(posterior_weight_n1.5, 4))
```

The last two lines have been showing each weight of each mixture member.
(n4 is for mean 4 model, and n1.5 is for mean 1.5 model.)
Note that, the member that is closest to true mean of data gets more weight for each case.

## step 2: select mixture with posterior weights

I will draw posterior samples by
- choosing mixture
- at the mixture member, draw a random sample

To avoid loop, let's choose one of the mixtures for 10000 sample first.

```{r}
N = 10000
chosen_mixture_n4 = sample(c(1,2,3), N, replace=TRUE, prob=posterior_weight_n4)
chosen_mixture_n1.5 = sample(c(1,2,3), N, replace=TRUE, prob=posterior_weight_n1.5)

chosen_mixture_count_n4 = c(
    sum(chosen_mixture_n4==1),
    sum(chosen_mixture_n4==2), 
    sum(chosen_mixture_n4==3))
print(chosen_mixture_count_n4)

chosen_mixture_count_n1.5 = c(
    sum(chosen_mixture_n1.5==1),
    sum(chosen_mixture_n1.5==2),
    sum(chosen_mixture_n1.5==3))
print(chosen_mixture_count_n1.5)
```

## step 3: get posterior samples using selected mixture lists

Using the choosing results, let's draw posterior samples (10000 samples for each case).

For the N(4,10) case,

```{r}
posterior_sample_vec_n4 = c()
for(i in 1:3){
    params = update_normalmixture_parameters(prior_mu_vec[i], mean(simed_obs_n4))
    posterior_sample_vec_n4 = c(
        posterior_sample_vec_n4, 
        rnorm(chosen_mixture_count_n4[i], params[1], params[2]))
}
hist(posterior_sample_vec_n4, breaks=100, freq=FALSE)
mean(posterior_sample_vec_n4)
var(posterior_sample_vec_n4)
```

Next, for the N(1.5,10) case,

```{r}
posterior_sample_vec_n1.5 = c()
for(i in 1:3){
    params = update_normalmixture_parameters(prior_mu_vec[i], mean(simed_obs_n1.5))
    posterior_sample_vec_n1.5 = c(
        posterior_sample_vec_n1.5, 
        rnorm(chosen_mixture_count_n1.5[i], params[1], params[2]))
}
hist(posterior_sample_vec_n1.5, breaks=100, freq=FALSE)
mean(posterior_sample_vec_n1.5)
var(posterior_sample_vec_n1.5)
```

They seem good! For each case, posterior mode gets closer to the true mean of the dataset.


# Draw posterior predictive samples

## way 1. using posterior samples

To begin with, the first approch is
- choose one posterior sample generated by the above.
- draw one predictive posterior sample following the data model (or the likelihood).

With the mean 4 case, let me draw 10000 posterior predictive samples.

```{r}
K=10000
chosen_mu = sample(posterior_sample_vec_n4, K, replace=TRUE)
posterior_predictive_sample_n4 = c()
for(i in 1:K){
    posterior_predictive_sample_n4 = c(posterior_predictive_sample_n4, rnorm(1, chosen_mu[i], sqrt(10)))
}
hist(posterior_predictive_sample_n4, breaks=100, freq=FALSE)
curve(dnorm(x, 4, sqrt(10)), add=TRUE)
mean(posterior_predictive_sample_n4)
var(posterior_predictive_sample_n4)
```

Note that curve on the histogram is the density of N(4,10). (not a smoothing curve!)

And, for the mean 1.5 case,

```{r}
chosen_mu = sample(posterior_sample_vec_n1.5, K, replace=TRUE)
posterior_predictive_sample_n1.5 = c()
for(i in 1:K){
    posterior_predictive_sample_n1.5 = c(posterior_predictive_sample_n1.5, rnorm(1, chosen_mu[i], sqrt(10)))
}
hist(posterior_predictive_sample_n1.5, breaks=100, freq=FALSE)
curve(dnorm(x, 1.5, sqrt(10)), add=TRUE)
mean(posterior_predictive_sample_n1.5)
var(posterior_predictive_sample_n1.5)
```

Again, the curve on the histogram is the density of N(1.5,10). (not a smoothing curve!)

## way 2. using posterior predictive distribution directly

Since we get the posterior predictive distrbution at 1-(d),
we can use it directly to draw posterior predictive samples.

Basically, this procedure is very similar to the way that I used for drawing posterior samples.
I will draw 10000 posterior predictive samples.
Let's choose mixture member with replacement as a beginning.

```{r}
K = 10000
chosen_mixture_n4 = sample(c(1,2,3), K, replace=TRUE, prob=posterior_weight_n4)
chosen_mixture_n1.5 = sample(c(1,2,3), K, replace=TRUE, prob=posterior_weight_n1.5)

chosen_mixture_count_n1.5 = c(
    sum(chosen_mixture_n1.5==1),
    sum(chosen_mixture_n1.5==2),
    sum(chosen_mixture_n1.5==3))
chosen_mixture_count_n1.5
chosen_mixture_count_n4 = c(
    sum(chosen_mixture_n4==1),
    sum(chosen_mixture_n4==2), 
    sum(chosen_mixture_n4==3))
chosen_mixture_count_n4

update_normalmixture_predictive_parameters <- function(mu, x_bar){
    tau = 1
    sigma = sqrt(10)
    n = 50

    scale_param  = 1/((n/sigma^2) + (1/tau^2))
    loc_param = scale_param * (x_bar*n/sigma^2 + mu/tau^2)
    return(c(loc_param, sqrt(scale_param + sigma^2)))
}

```

Next, draw each posterior predictive sample at the chosen member of mixture.

For the mean 4 case,

```{r}
postpredictive_sample_vec_n4_v2 = c()
for(i in 1:3){
    params = update_normalmixture_predictive_parameters(
        prior_mu_vec[i], mean(simed_obs_n4))
    postpredictive_sample_vec_n4_v2 = c(
        postpredictive_sample_vec_n4_v2, 
        rnorm(chosen_mixture_count_n4[i], params[1], params[2]))
}
hist(postpredictive_sample_vec_n4_v2, breaks=100, freq=FALSE)
curve(dnorm(x, 4, sqrt(10)), add=TRUE)
mean(postpredictive_sample_vec_n4_v2)
var(postpredictive_sample_vec_n4_v2)

```

Note that curve on the histogram is the density of N(4, 10).
We can observe that the mixture model works well.

For the mean 1.5 case,

```{r}
postpredictive_sample_vec_n1.5_v2 = c()
for(i in 1:3){
    params = update_normalmixture_predictive_parameters(
        prior_mu_vec[i], mean(simed_obs_n1.5))
    postpredictive_sample_vec_n1.5_v2 = c(
        postpredictive_sample_vec_n1.5_v2, 
        rnorm(chosen_mixture_count_n1.5[i], params[1], params[2]))
}
hist(postpredictive_sample_vec_n1.5_v2, breaks=100, freq=FALSE)
curve(dnorm(x, 1.5, sqrt(10)), add=TRUE)
mean(postpredictive_sample_vec_n1.5_v2)
var(postpredictive_sample_vec_n1.5_v2)
```

Note that curve on the histogram is the density of N(1.5, 10).